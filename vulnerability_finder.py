import pandapower as pp
import pandapower.networks as pn
import numpy as np
import torch
import itertools
import time
from tqdm import tqdm
from collections import defaultdict
import argparse
import os
import copy

from PowerGridEnv import PowerGridEnv
from Gnn import GNN
from Train import PPOAgent, obs_to_data

class VulnerabilityFinder:
    """
    Finds critical vulnerabilities in power grid systems by exhaustively testing
    combinations of line removals and measuring their impact according to the 
    PowerGridEnv reward structure.
    """
    
    def __init__(self, case_name, k=3, vmin=0.95, vmax=1.05, max_loading=1.0):
        """
        Initialize the vulnerability finder for a specific case.
        
        Args:
            case_name (str): Name of the pandapower case to analyze (e.g., 'case14')
            k (int): Maximum number of lines to remove
            vmin (float): Minimum acceptable voltage (p.u.)
            vmax (float): Maximum acceptable voltage (p.u.)
            max_loading (float): Maximum acceptable line loading (ratio)
        """
        self.case_name = case_name
        self.k = k
        self.vmin = vmin
        self.vmax = vmax
        self.max_loading = max_loading
        
        # Load the original network
        self.orig_net = getattr(pn, case_name)()
        pp.runpp(self.orig_net)
        
        # Get line IDs
        self.lines = list(self.orig_net.line.index)
        self.num_lines = len(self.lines)
        self.num_buses = len(self.orig_net.bus)
        
        # Store results
        self.vulnerabilities = []
        
    def evaluate_combination(self, removed_lines):
        """
        Evaluate the impact of removing a specific set of lines.
        
        Args:
            removed_lines (list): List of line IDs to remove
            
        Returns:
            dict: Results including vulnerability score and details
        """
        # Create a copy of the original network
        net = copy.deepcopy(self.orig_net)
        
        # Remove the lines
        for line in removed_lines:
            net.line.at[line, 'in_service'] = False
        
        # Try to run power flow
        collapsed = False
        islanded = False
        
        try:
            pp.runpp(net)
            # Check for islands (NaN voltage values)
            if np.isnan(net.res_bus.vm_pu.values).any():
                islanded = True
        except pp.LoadflowNotConverged:
            collapsed = True
            
        # Calculate voltage violations
        bus_v = net.res_bus.vm_pu.values if not collapsed else np.zeros(self.num_buses)
        
        # Calculate voltage deviations like in PowerGridEnv
        v_deviations = np.zeros_like(bus_v)
        for i, v in enumerate(bus_v):
            if not np.isnan(v):
                if v < self.vmin:
                    v_deviations[i] = (self.vmin - v) / self.vmin
                elif v > self.vmax:
                    v_deviations[i] = (v - self.vmax) / self.vmax
        
        bus_violation = np.sum(v_deviations ** 2) / len(bus_v)
        
        # Calculate line loading violations if not collapsed
        if not collapsed and not islanded:
            loadings = net.res_line.loading_percent.values / 100.0
            l_deviations = np.maximum(0, loadings - self.max_loading)
            line_violation = np.sum(l_deviations ** 2) / len(loadings)
        else:
            line_violation = 0
        
        # Calculate final score based on PowerGridEnv priorities
        # Higher score means more vulnerable
        if collapsed:
            score = 100.0  # Highest priority
        elif islanded:
            score = 25.0  # Second priority
        else:
            # Weighted sum of violations similar to PowerGridEnv
            score = (bus_violation + 1.5 * line_violation) * 10.0  # Scale factor for clarity
        
        score *= (0.90 ** (len(removed_lines)-1))  # Penalize for more lines removed
        
        return {
            'removed_lines': removed_lines.copy(),
            'score': score,
            'collapsed': collapsed,
            'islanded': islanded,
            'bus_violation': bus_violation,
            'line_violation': line_violation
        }
    
    def find_vulnerabilities(self, max_combinations=None):
        """
        Find vulnerabilities by testing combinations of line removals.
        
        Args:
            max_combinations (int, optional): Limit the number of combinations to test
            
        Returns:
            list: Sorted vulnerabilities by severity
        """
        results = []
        
        # Generate all combinations of line removals up to k
        all_combinations = []
        for i in range(1, self.k + 1):
            all_combinations.extend(list(itertools.combinations(self.lines, i)))
        
        # Limit combinations if requested
        if max_combinations and len(all_combinations) > max_combinations:
            print(f"Limiting to {max_combinations} random combinations from {len(all_combinations)} total")
            np.random.shuffle(all_combinations)
            all_combinations = all_combinations[:max_combinations]
        
        # Test each combination
        total = len(all_combinations)
        print(f"Testing {total} combinations of line removals for {self.case_name}")
        
        for combination in tqdm(all_combinations):
            result = self.evaluate_combination(list(combination))
            results.append(result)
            
        # Sort by vulnerability score (descending)
        results.sort(key=lambda x: x['score'], reverse=True)
        self.vulnerabilities = results
        
        return results
    
    def display_results(self, top_n=10):
        """Display the top vulnerabilities found"""
        print(f"\nTop {min(top_n, len(self.vulnerabilities))} vulnerabilities for {self.case_name}:")
        print("=" * 80)
        
        for i, v in enumerate(self.vulnerabilities[:top_n]):
            status = "COLLAPSED" if v['collapsed'] else "ISLANDED" if v['islanded'] else "STRESSED"
            lines_str = ", ".join(map(str, v['removed_lines']))
            
            print(f"{i+1}. Score: {v['score']:.2f} - Status: {status}")
            print(f"   Lines removed: {lines_str}")
            if not v['collapsed'] and not v['islanded']:
                print(f"   Bus violations: {v['bus_violation']:.4f}, Line violations: {v['line_violation']:.4f}")
            print("-" * 80)
    
    def save_results(self, filename):
        """Save results to a file"""
        with open(filename, 'w') as f:
            f.write(f"Vulnerability Analysis for {self.case_name}, k={self.k}\n")
            f.write("=" * 80 + "\n\n")
            
            for i, v in enumerate(self.vulnerabilities):
                status = "COLLAPSED" if v['collapsed'] else "ISLANDED" if v['islanded'] else "STRESSED"
                lines_str = ", ".join(map(str, v['removed_lines']))
                
                f.write(f"{i+1}. Score: {v['score']:.2f} - Status: {status}\n")
                f.write(f"   Lines removed: {lines_str}\n")
                if not v['collapsed'] and not v['islanded']:
                    f.write(f"   Bus violations: {v['bus_violation']:.4f}, Line violations: {v['line_violation']:.4f}\n")
                f.write("-" * 80 + "\n\n")

def find_model_top_vulnerabilities_comprehensive(model_path, case_name, k=3, num_vulnerabilities=5):
    """
    Find vulnerabilities by exploring the top 5 actions at each step.
    Only skips critical lines when encountered during exploration.
    
    Args:
        model_path (str): Path to the trained model
        case_name (str): Case name to test
        k (int): Maximum number of lines to remove
        num_vulnerabilities (int): Number of top vulnerabilities to return
        
    Returns:
        list: Top vulnerabilities identified by the model
    """
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Create environment with specified case
    env = PowerGridEnv(case_list=[case_name], k=k)
    
    # Load model
    model = GNN(node_feat_dim=4, edge_feat_dim=5, hidden_dim=64, n_layers=3)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # List to store all vulnerability candidates
    vulnerability_candidates = []
    
    # Keep track of lines already selected to avoid duplicates
    selected_line_sets = set()
    
    # Keep track of lines that are known to cause collapse on their own
    # This gets populated during exploration
    known_critical_lines = set()
    
    print(f"Finding vulnerabilities by exploring the model-guided tree...")
    
    # Reset environment once
    initial_obs, _ = env.reset()
    
    # Define a recursive function to explore all paths in the 5^k tree
    def explore_all_paths(obs, lines_removed=None, depth=0):
        if lines_removed is None:
            lines_removed = []
            
        # Check if we've reached the end of the path or episode is done
        if depth == k:
            # Evaluate this path if we haven't seen it before
            lines_tuple = tuple(sorted(lines_removed))
            if lines_tuple not in selected_line_sets:
                selected_line_sets.add(lines_tuple)
                
                # Evaluate the vulnerability score
                finder = VulnerabilityFinder(case_name, k=k)
                result = finder.evaluate_combination(list(lines_removed))
                vulnerability_candidates.append(result)
                
                print(f"Found vulnerability candidate {len(vulnerability_candidates)}: Lines {lines_removed}")
                status = "COLLAPSED" if result["collapsed"] else "ISLANDED" if result["islanded"] else "STRESSED"
                print(f"  Status: {status}, Score: {result['score']:.2f}")
                
            return
        
        # Get top actions from the model for this state
        data = obs_to_data(obs, device=device)
        with torch.no_grad():
            logits, _ = model(data)
            mask = torch.tensor(obs["action_mask"], dtype=torch.bool, device=device)
            masked_logits = logits.masked_fill(~mask, -1e8)
            
            # Always get top 5 actions (or fewer if not enough valid actions)
            n_actions = min(5, sum(mask).item()) 
            
            # Skip if no valid actions
            if n_actions == 0:
                return
                
            top_values, top_indices = torch.topk(masked_logits, n_actions)
            
        # First, check the current state (lines already removed)
        # Check if we already have a collapse with current lines
        if lines_removed:
            current_lines_tuple = tuple(sorted(lines_removed))
            # Skip evaluation if we've already seen this exact combination
            if current_lines_tuple not in selected_line_sets:
                finder = VulnerabilityFinder(case_name, k=len(lines_removed))
                current_result = finder.evaluate_combination(list(lines_removed))
                
                # Record this vulnerability
                vulnerability_candidates.append(current_result)
                selected_line_sets.add(current_lines_tuple)
                
                print(f"Found intermediate vulnerability {len(vulnerability_candidates)}: Lines {lines_removed}")
                status = "COLLAPSED" if current_result["collapsed"] else "ISLANDED" if current_result["islanded"] else "STRESSED"
                print(f"  Status: {status}, Score: {current_result['score']:.2f}")
                
                # If current state already collapsed, we don't need to explore further
                if current_result["collapsed"] or current_result["islanded"]:
                    return
        
        # Now explore next actions
        for i, action in enumerate(top_indices):
            action = action.item()
            
            # Get the line ID for this action
            line_id = env.all_line_ids[action]
            
            # Skip this action if we know from previous exploration that this single line
            # causes collapse on its own and we have other lines to explore
            if line_id in known_critical_lines and depth > 0 and i < len(top_indices) - 1:
                print(f"  Skipping known critical line {line_id} (discovered during exploration)")
                continue
                
            # Make a copy of the environment
            try:
                env_copy = copy.deepcopy(env)
                env_copy.load_state(obs)
            except Exception as e:
                print(f"Error copying environment: {e}")
                continue
            
            # Add to our path
            new_lines = lines_removed + [line_id]
            
            # Skip if we've already evaluated this exact combination
            if tuple(sorted(new_lines)) in selected_line_sets:
                continue
            
            # Take the action
            try:
                new_obs, reward, done, _, info = env_copy.step(action)
            except Exception as e:
                print(f"Error taking step: {e}")
                continue
            
            # Evaluate this new state
            lines_tuple = tuple(sorted(new_lines))
            
            if lines_tuple not in selected_line_sets:
                selected_line_sets.add(lines_tuple)
                
                finder = VulnerabilityFinder(case_name, k=len(new_lines))
                result = finder.evaluate_combination(list(new_lines))
                vulnerability_candidates.append(result)
                
                print(f"Found vulnerability candidate {len(vulnerability_candidates)}: Lines {new_lines}")
                status = "COLLAPSED" if result["collapsed"] else "ISLANDED" if result["islanded"] else "STRESSED"
                print(f"  Status: {status}, Score: {result['score']:.2f}")
                
                # If this is a single line and it causes collapse, mark it as a critical line
                if len(new_lines) == 1 and (result["collapsed"] or result["islanded"]):
                    known_critical_lines.add(new_lines[0])
                    print(f"  Identified line {new_lines[0]} as critical (causes collapse alone)")
                
                # If this combination collapsed the grid, no need to explore deeper with these lines
                if result["collapsed"] or result["islanded"]:
                    continue
            
            # Continue exploring this branch ONLY if:
            # 1. We haven't reached max depth
            # 2. The grid hasn't collapsed with current lines
            if depth + 1 < k:
                explore_all_paths(new_obs, new_lines, depth + 1)
    
    # Start exploration
    try:
        explore_all_paths(initial_obs)
        print(f"Completed exploration. Found {len(vulnerability_candidates)} unique vulnerabilities.")
        
        if known_critical_lines:
            print(f"Discovered {len(known_critical_lines)} critical individual lines during exploration:")
            print(f"Lines: {sorted(list(known_critical_lines))}")
    except Exception as e:
        print(f"Error during exploration: {e}")
        print("Falling back to sequential model runs...")
        return find_model_top_vulnerabilities_sequential(model_path, case_name, k, num_vulnerabilities)
    
    # Sort by vulnerability score
    vulnerability_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # Take top vulnerabilities
    top_vulnerabilities = vulnerability_candidates[:num_vulnerabilities]
    
    # Display results
    print("\nTop vulnerabilities found by model:")
    print("=" * 80)
    
    for i, v in enumerate(top_vulnerabilities):
        status = "COLLAPSED" if v['collapsed'] else "ISLANDED" if v['islanded'] else "STRESSED"
        lines_str = ", ".join(map(str, v['removed_lines']))
        
        print(f"{i+1}. Score: {v['score']:.2f} - Status: {status}")
        print(f"   Lines removed: {lines_str}")
        if not v['collapsed'] and not v['islanded']:
            print(f"   Bus violations: {v['bus_violation']:.4f}, Line violations: {v['line_violation']:.4f}")
        print("-" * 80)
    
    return top_vulnerabilities

# Fallback function that uses the original sequential approach
def find_model_top_vulnerabilities_sequential(model_path, case_name, k=3, num_vulnerabilities=5):
    # Original implementation (renamed to avoid recursion)
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Create environment with specified case
    env = PowerGridEnv(case_list=[case_name], k=k)
    
    # Load model
    model = GNN(node_feat_dim=4, edge_feat_dim=5, hidden_dim=64, n_layers=3)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # List to store top vulnerability candidates
    vulnerability_candidates = []
    
    # Keep track of lines already selected to avoid duplicates
    selected_line_sets = set()
    
    # Repeat until we have enough unique vulnerabilities or max iterations reached
    max_iterations = 30  # Just to prevent infinite loops
    iterations = 0
    
    while len(vulnerability_candidates) < num_vulnerabilities and iterations < max_iterations:
        iterations += 1
        
        # Reset environment
        obs, _ = env.reset()
        done = False
        episode_lines = []
        
        # Run until done or reached k removals
        while not done and len(episode_lines) < k:
            data = obs_to_data(obs, device=device)
            
            with torch.no_grad():
                logits, _ = model(data)
                mask = torch.tensor(obs["action_mask"], dtype=torch.bool, device=device)
                masked_logits = logits.masked_fill(~mask, -1e8)
                
                # Get top action but introduce some randomness
                if np.random.random() < 0.3:  # 30% chance to explore
                    probs = torch.softmax(masked_logits, dim=0)
                    action = torch.multinomial(probs, 1).item()
                else:
                    action = torch.argmax(masked_logits).item()
            
            # Get the line that was removed
            line_id = env.all_line_ids[action]
            episode_lines.append(line_id)
            
            # Take action
            obs, reward, done, _, info = env.step(action)
        
        # Only add if this set of lines hasn't been seen before
        episode_lines_tuple = tuple(sorted(episode_lines))
        if episode_lines_tuple not in selected_line_sets:
            selected_line_sets.add(episode_lines_tuple)
            
            # Evaluate the vulnerability to get its score
            finder = VulnerabilityFinder(case_name, k=k)
            result = finder.evaluate_combination(list(episode_lines))
            
            vulnerability_candidates.append(result)
            
            print(f"Found vulnerability candidate {len(vulnerability_candidates)}: Lines {episode_lines}")
            status = "COLLAPSED" if result["collapsed"] else "ISLANDED" if result["islanded"] else "STRESSED"
            print(f"  Status: {status}, Score: {result['score']:.2f}")
    
    # Sort by vulnerability score
    vulnerability_candidates.sort(key=lambda x: x['score'], reverse=True)
    
    # Take top vulnerabilities
    top_vulnerabilities = vulnerability_candidates[:num_vulnerabilities]
    
    return top_vulnerabilities

def main():
    parser = argparse.ArgumentParser(description="Find power grid vulnerabilities and evaluate model")
    parser.add_argument("--case", type=str, default="case14", help="Power grid case name")
    parser.add_argument("--k", type=int, default=3, help="Maximum number of lines to remove")
    parser.add_argument("--max_combos", type=int, default=None, help="Maximum combinations to test")
    parser.add_argument("--model", type=str, default="best_power_grid_model.pt", help="Path to trained model")
    parser.add_argument("--top_n", type=int, default=10, help="Number of top vulnerabilities to display")
    parser.add_argument("--model_vulnerabilities", type=int, default=5, 
                       help="Number of top vulnerabilities to find directly from model")
    parser.add_argument("--mode", type=str, default="both", 
                       choices=["exhaustive", "model", "both"], 
                       help="Analysis mode: exhaustive, model-only, or both")
    parser.add_argument("--comprehensive", action="store_true", 
                       help="Use comprehensive 5^k search instead of pruned search")
    args = parser.parse_args()
    
    # Create results directory if it doesn't exist
    os.makedirs("results", exist_ok=True)
    
    # Determine which analysis to run
    run_exhaustive = args.mode in ["exhaustive", "both"]
    run_model_analysis = args.mode in ["model", "both"] and os.path.exists(args.model)
    
    # Dictionary to store all vulnerabilities found
    all_vulnerabilities = {}
    
    # Run exhaustive analysis if requested
    if run_exhaustive:
        start_time = time.time()
        finder = VulnerabilityFinder(args.case, k=args.k)
        vulnerabilities = finder.find_vulnerabilities(max_combinations=args.max_combos)
        elapsed = time.time() - start_time
        
        print(f"\nExhaustive analysis completed in {elapsed:.1f} seconds")
        
        # Display and save results
        finder.display_results(top_n=args.top_n)
        results_file = f"results/{args.case}_k{args.k}_vulnerabilities.txt"
        finder.save_results(results_file)
        print(f"Saved detailed results to {results_file}")
        
        # Store the vulnerabilities
        all_vulnerabilities["exhaustive"] = vulnerabilities
    
    # Run model-based analysis if requested
    if run_model_analysis:
        print(f"\nFinding top vulnerabilities with model {args.model}...")
        model_vulnerabilities = find_model_top_vulnerabilities_comprehensive(
            args.model, args.case, args.k, args.model_vulnerabilities
        )
        
        # Save model vulnerabilities
        model_results_file = f"results/{args.case}_model_vulnerabilities.txt"
        with open(model_results_file, 'w') as f:
            f.write(f"Model-identified Vulnerabilities for {args.case}, k={args.k}\n")
            f.write("=" * 80 + "\n\n")
            
            for i, v in enumerate(model_vulnerabilities):
                status = "COLLAPSED" if v['collapsed'] else "ISLANDED" if v['islanded'] else "STRESSED"
                lines_str = ", ".join(map(str, v['removed_lines']))
                
                f.write(f"{i+1}. Score: {v['score']:.2f} - Status: {status}\n")
                f.write(f"   Lines removed: {lines_str}\n")
                if not v['collapsed'] and not v['islanded']:
                    f.write(f"   Bus violations: {v['bus_violation']:.4f}, Line violations: {v['line_violation']:.4f}\n")
                f.write("-" * 80 + "\n\n")
                
        print(f"Saved model vulnerabilities to {model_results_file}")
        
        # Store the vulnerabilities
        all_vulnerabilities["model"] = model_vulnerabilities
    
    # Compare results if both analyses were run
    if run_exhaustive and run_model_analysis:
        print("\nComparing exhaustive vs. model-found vulnerabilities:")
        print("=" * 80)
        
        # Get the top vulnerabilities from the exhaustive search
        top_exhaustive = all_vulnerabilities["exhaustive"][:args.top_n]
        top_exhaustive_sets = [set(v["removed_lines"]) for v in top_exhaustive]
        
        # Check if model found any of the top vulnerabilities
        found_matches = []
        for mv in model_vulnerabilities:
            model_set = set(mv["removed_lines"])
            for i, ev_set in enumerate(top_exhaustive_sets):
                if model_set == ev_set:
                    found_matches.append((mv, top_exhaustive[i], i+1))
        
        # Display results
        if found_matches:
            print(f"Model successfully identified {len(found_matches)} of the top {args.top_n} vulnerabilities:")
            for model_vuln, exhaustive_vuln, rank in found_matches:
                print(f"- Found vulnerability ranked #{rank} in exhaustive search")
                print(f"  Lines: {model_vuln['removed_lines']}, Score: {model_vuln['score']:.2f}")
                print(f"  Exhaustive Score: {exhaustive_vuln['score']:.2f}")
                print()
        else:
            print(f"Model did not identify any of the top {args.top_n} vulnerabilities exactly.")
        
        # Save comparison results
        comparison_file = f"results/{args.case}_model_comparison.txt"
        with open(comparison_file, 'w') as f:
            f.write(f"Comparison of Model vs Exhaustive Search for {args.case}, k={args.k}\n")
            f.write("=" * 80 + "\n\n")
            
            if found_matches:
                f.write(f"Model successfully identified {len(found_matches)} of the top {args.top_n} vulnerabilities:\n")
                for model_vuln, exhaustive_vuln, rank in found_matches:
                    f.write(f"- Found vulnerability ranked #{rank} in exhaustive search\n")
                    f.write(f"  Lines: {model_vuln['removed_lines']}, Score: {model_vuln['score']:.2f}\n")
                    f.write(f"  Exhaustive Score: {exhaustive_vuln['score']:.2f}\n\n")
            else:
                f.write(f"Model did not identify any of the top {args.top_n} vulnerabilities exactly.\n")
        
        print(f"Saved comparison results to {comparison_file}")

if __name__ == "__main__":
    main()
